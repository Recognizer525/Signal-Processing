\documentclass[11pt]{article}
\usepackage[english,russian]{babel}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, left=2.5cm, right=1.5cm, top=2.5cm, bottom=2.5cm]{geometry}
\usepackage{animate} 
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{longtable}
\usepackage{amssymb}
\usepackage{physics}
\usepackage{tikz}
\usepackage{comment}
\usepackage{animate} 
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{longtable}
\usepackage{amssymb}
\usepackage{physics}
\usepackage{tikz}
\usepackage{comment}
\usepackage{colortbl}
%\usepackage{xcolor}
\usepackage[normalem]{ulem}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{cancel}
\usepackage{mathtools}
\usepackage[most]{tcolorbox}
\usepackage[mathscr]{euscript}

\usepackage[dvipsnames]{xcolor}
\usepackage{amsfonts}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
 
\newcommand{\Expect}{\mathbb{E}}
\newcommand{\Var}{\mathcal{D}}
\newcommand{\Cov}{\mathsf{cov}}
\newcommand{\Norm}{\mathcal{N}}
\newcommand{\NormComplex}{\mathcal{CN}}
\newcommand{\Natural}{\mathbb{N}}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\Complex}{\mathbb{C}}
\newcommand{\Int}{\mathbb{Z}}
\newcommand{\DK}{\mathbf{D}_{KL}}
\DeclarePairedDelimiterX{\infdivx}[2]{(}{)}{%
  #1\;\delimsize\|\;#2%
}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\Det}{Det}
\newcommand{\infdiv}{D_{KL}\infdivx}
\newcommand\Fontvi{\fontsize{8.2}{7.2}\selectfont}
\newcommand\Fontvia{\fontsize{9}{8}\selectfont}
\newcommand\Fontvib{\fontsize{10.8}{9.6}\selectfont}
\newcommand\Fontvic{\fontsize{8.0}{7.0}\selectfont}
\newcommand{\myitem}{\item[\checkmark]}
%\newcommand{\myitem}{\item[\squares]}

\begin{document}
\begin{center}
\fontsize{20}{23}\selectfont \color{red}{\textbf{ЕМ-алгоритм, стохастическая модель сигнала}}
\end{center}
\begin{center}
\fontsize{16}{20}\selectfont \color{teal}{\textbf{Постановка задачи}}
\end{center}
Предположим, имеется линейная антенная решетка, состоящая из $\text{L}$ сенсоров, которая принимает сигналы, направленные из $\text{K}$  источников, причем $\text{K} < \text{L}$. Этим источникам соответствуют угловые координаты (DoA) $\theta$, практически не изменяющиеся во времени. По итогам измерений было получено $\text{T}$ снимков полученного сигнала, причем ввиду стохастических технических сбоев, связанных с сенсорами, большая часть таких снимков содержит помимо надежных данных ненадежные, которые в рамках данной задачи рассматриваются как пропуски. Пусть $X$ --- набор наблюдений, полученных сенсорами в моменты времени $t=1,\ldots,\text{T}$, $X_t$ соответствует наблюдению в момент времени $t$, через $x$ и $x_t$ будем обозначать реализации полного набора наблюдений и наблюдения в отдельный момент времени $t$  соответственно. Ввиду наличия пропусков в данных, будем считать, что набор наблюдений $X$ состоит из доступной части $X_o = \{X_{t, o_t}\}_{t=1}^\text{T}$ и недоступной: $X_m = \{X_{t, m_t}\}_{t=1}^\text{T}$, причем $o_t \cup m_t = \{1,\ldots,\text{T}\}, o_t \cap m_t = \varnothing,  \forall t \in \{1,\ldots,\text{T}\}$. Предполагается, что $\nexists o_t: o_t = \varnothing$, т.е. нет таких наблюдений, которые состоят лишь из недоступной части. \\ Набор $X$ является результатом следующей модели наблюдений:
\begin{equation}
X = \text{A} S + N,
\end{equation}
где
\begin{equation}
X=[X_1,\ldots, X_\text{T}], S=[S_1,\ldots, S_\text{T}], N=[N_1,\ldots, N_\text{T}],
\end{equation}
и $X_t \in \Complex^{\text{L} \times \text{T}}, S_t \in \Complex^{\text{K} \times \text{T}}, N_t \in \Complex^{\text{L} \times \text{T}}$ -- векторы-столбцы, соответствующие наблюдениям, источникам и шумам в момент времени  $t = 1, \ldots, \text{T}$, $\text{A}$ -- матрица векторов направленности для равномерного линейного массива:
\begin{equation}
\text{A}(\theta) = \begin{bmatrix}
1&1&\dots&1\\
e^{-2j\pi \frac{\text{d}}{\lambda}\sin(\theta_1)}& e^{-2j\pi \frac{\text{d}}{\lambda}\sin(\theta_2)}&\dots&e^{-2j\pi \frac{\text{d}}{\lambda}\sin(\theta_\text{K})}\\
\dots&\dots&\ddots&\vdots\\
e^{-2j\pi (\text{L}-1) \frac{\text{d}}{\lambda}\sin(\theta_1)}& e^{-2j\pi (\text{L}-1) \frac{\text{d}}{\lambda}\sin(\theta_2)}&\dots&e^{-2j\pi (\text{L}-1) \frac{\text{d}}{\lambda}\sin(\theta_\text{K})}\\
\end{bmatrix}.
\end{equation}
Сигналы, испускаемые источниками, шумы на сенсорах и наблюдения предполагаются стохастическими: $S_t \sim \NormComplex(\mathbf{O}_{\text{K} \times 1},\mathbf{\Gamma})$, $N_t \sim \NormComplex(\mathbf{O}_{\text{L} \times 1}, \mathbf{\Lambda})$, $ X_t \sim \NormComplex(\mathbf{O}_{\text{L} \times 1}, \text{A}\mathbf{\Gamma}\text{A}^* + \mathbf{\Lambda})$, $t=1,\ldots,\text{T}$. Матрицы $\mathbf{\Gamma}$ и $\mathbf{\Lambda}$ предполагаются диагональными, т.е. сигналы не коррелированы между собой, шумы также не коррелированы между собой, корреляция между сигналами и шумами также отсутствует. Сигналы предполагаются узкополосными.
Составим EM-алгоритм для случая известного шума.
\clearpage
\begin{center}
\fontsize{16}{20}\selectfont \color{teal}{\textbf{ЕМ-алгоритм для известного шума}}
\end{center}
Воспользуемся EM-алгоритмом для того, чтобы определить значения параметров $\Upsilon = (\theta, \mathbf{\Gamma})$, пропущенные значения $X_m=\{X_{t, m_t}\}_{t=1}^\text{T}$ и сигналы $S$ рассматриваются как латентные переменные. Наблюдения $X_t$, $t=1,\ldots,\text{T}$ предполагаются независимыми и одинаково распределенными.
\begin{center}
\fontsize{14}{18}\selectfont \color{red}{\textbf{Е-шаг}}
\end{center}
Требуется найти математическое ожидание полного правдоподобия с учетом  текущей оценки параметров и апостериорного совместного распределения пропущенных значений в наблюдениях $X_m$ и сигналов $S$ 
\begin{equation}
 \Expect_{(X_m,S)|X_o=x_o, \Upsilon^{(\tau-1)}}[\log P(X, S)].
\end{equation}
Преобразуем выражение, учитывая тот факт, что наблюдения являются независимыми, и обозначив через $\mathcal{I}$ условную информацию $X_o=x_o, \Upsilon^{(\tau-1)}$ и через $\mathcal{I}_t$ условную информацию $X_{t,o_t}=x_{t,o_t}, \Upsilon^{(\tau-1)}$:
\begin{equation*}
\begin{gathered}
 \Expect_{(X_m,S)|\mathcal{I}}[\log P(X, S)] = \\
 \Expect_{(X_m,S)|\mathcal{I}}[\log P(X|S) + \log P(S)] = \\
\Big[\sum_{t=1}^T  \Expect_{(X_{t,m_t},S_t)|\mathcal{I}_t} \log [P(X_t|S_t)] + \sum_{t=1}^T \Expect_{(X_{t,m_t},S_t)|\mathcal{I}_t}[\log P(S_t)]\Big] = \\
- \text{T} \Big[\log|\mathbf{\Lambda}| +\Tr \big( \mathbf{\Lambda}^{-1} \Expect_{(X_m,S)|\mathcal{I}}[XX^*] \big) - 2 \Tr\big(\mathbf{\Lambda}^{-1} \text{A}(\theta) \Expect_{(X_m,S)|\mathcal{I}}[X S^*]\big) \\
+ \Tr(\mathbf{\Lambda}^{-1} \text{A}(\theta) \Expect_{(X_m,S)|\mathcal{I}}[SS^*] \text{A}^*(\theta)) + \log |\mathbf{\Gamma}| + \Tr(\mathbf{\Gamma}^{-1} \Expect_{(X_m,S)|\mathcal{I}}[SS^*]) \Big]. 
\end{gathered}
\end{equation*}
Выражение $\Expect_{(X_{t,m_t},S_t)|\mathcal{I}_t}[\cdot]$  соответствуют случаю максимально полного набора латентных переменных: некоторые наблюдения в наборе могут не содержать пропуски ($X_{t,m_t}=\varnothing$), и тогда это выражение упрощается до $\Expect_{S_t|\mathcal{I}_t}[\cdot]$.
Для нахождения условных моментов, указанных в формуле выше, требуется найти апостериорное распределение скрытых переменных. 
\begin{center}
\fontsize{12}{16}\selectfont \color{Purple}{\textbf{Апостериорное распределение латентных переменных для неполных наблюдений}}
\end{center}
Воспользуемся формулой произведения плотностей:
\begin{gather}
P(X_{t,m_t},S_t|\mathcal{I}_t) = P(X_{t,m_t}|\mathcal{I}_t) \cdot P(S_t|X_{t,m_t}, \mathcal{I}_t). 
\end{gather}
Сначала найдем апостериорное распределение $P(X_m|\mathcal{I})$. Для достижения этой цели, для каждой пары $ \{ (o_t, m_t): m_t \ne \varnothing \}$ создадим разбиение оценки ковариационной матрицы наблюдений $\hat{\text{R}}$ на блоки, индуцированное этим разбиением множества индексов, оно имеет следующий вид:
\begin{equation}
\hat{\text{R}} =
\begin{pmatrix}
\hat{\text{R}}_{o_t, o_t} & \hat{\text{R}}_{o_t, m_t}\\
\hat{\text{R}}_{m_t, o_t} & \hat{\text{R}}_{m_t, m_t}
\end{pmatrix},
\end{equation}
где каждый блок определяется как
\begin{equation*}
\hat{\text{R}}_{a,b} = (\hat{\text{R}}_{ij})_{i \in a, j \in b}.
\end{equation*}
Для каждого наблюдения, содержащего пропуски, требуется найти апостериорное распределение пропущенных значений, $P(X_{t, m_t}|X_{t, o_t}=x_{t, o_t},\Upsilon^{(\tau-1)}), t=1,\ldots,\text{T}, m_t \ne \varnothing$ . Обозначим через $\mathcal{I}_t$ условную информацию $X_{t, o_t}=x_{t, o_t},\Upsilon^{(\tau-1)}$.
Параметры апостериорного распределения $P(X_{t, m_t}|\mathcal{I}_t), t=1,\ldots,\text{T}$ на итерации $\tau$ можно найти следующим образом:
\begin{equation}
\left\{ \begin{aligned} 
\Expect[X_{t, m_t}|\mathcal{I}_t] &= \hat{\text{R}}_{m_t, o_t}\left(\hat{\text{R}}_{o_t, o_t}\right)^{-1}\cdot x_{t, o_t}, \\
\text{Cov}(X_{t, m_t}|\mathcal{I}_t) &= \hat{\text{R}}_{m_t, m_t}-\hat{\text{R}}_{m_t, o_t}\left(\hat{\text{R}}_{o_t, o_t}\right)^{-1}\hat{\text{R}}_{o_t, m_t},
\end{aligned} \right.
\end{equation}
где $\hat{\text{R}}_{o_t, o_t}= \hat{\text{R}}_{o_t, o_t}^{(\tau-1)}$, 
$\hat{\text{R}}_{o_t, m_t}= \hat{\text{R}}_{o_t, m_t}^{(\tau-1)}$, 
$\hat{\text{R}}_{m_t, o_t}= \hat{\text{R}}_{m_t, o_t}^{(\tau-1)}$,
$\hat{\text{R}}_{m_t, m_t}= \hat{\text{R}}_{m_t, m_t}^{(\tau-1)}$.\\
Для каждого наблюдения $X_t$, содержащего пропуски, оценим условный второй начальный момент $\Expect[X_t X_t^*|\mathcal{I}_t]$:
\begin{equation*}
\begin{gathered}
\Expect[X_t X_t^*|\mathcal{I}_t] = \Expect [X_t | \mathcal{I}_t] \cdot \Expect [X_t^* | \mathcal{I}_t] + \text{Cov} (X_t | \mathcal{I}_t) \\
= \Expect [X_t | \mathcal{I}_t] \cdot \Expect [X_t^* | \mathcal{I}_t] +
\begin{pmatrix}
\mathbf{O}_{o_t, o_t} & \mathbf{O}_{o_t, m_t} \\
\mathbf{O}_{m_t, o_t} & \text{Cov}(X_{t, m_t}|\mathcal{I}_t) \\
\end{pmatrix},
\end{gathered}
\end{equation*}
где $\mathbf{O}_{o_t, o_t}$, $\mathbf{O}_{o_t, m_t}$, $\mathbf{O}_{m_t, o_t}$ -- нулевые блочные матрицы. Разбиение указанной матрицы на четыре блока, три из которых состоят из нулей, индукцировано разбиением множества индексов $\{1,\ldots,\text{L}\}$ на множества $o_t, m_t$. \\
Нам недоступны значения $X_{t,m_t}$, ввиду этого мы не можем найти $P(S_t|\mathcal{I}_t, X_{t,m_t})$, однако ввиду того, что модель линейная гауссовская, мы можем найти параметры распределения $P(S_t|\mathcal{I}_t), t = 1,\ldots, \text{T}$.
Параметры апостериорного распределения $P(S_t|\mathcal{I}_t), t = 1,\ldots, \text{T}$ можно найти исходя из следующих формул, если $m_t \ne \varnothing$:
\begin{equation}
\left\{ \begin{aligned} 
\Expect[S_t|\mathcal{I}_t] &= \mathbf{\Gamma}^*\text{A}^* \hat{\text{R}}^{-1}\Expect [X_t | \mathcal{I}_t], \\
\text{Cov}(S_t|\mathcal{I}_t) &= \mathbf{\Gamma} - \mathbf{\Gamma}^*\text{A}^*  \hat{\text{R}}^{-1}\text{A}\mathbf{\Gamma} + \mathbf{\Gamma}\text{A}^* \hat{\text{R}}^{-1}\text{Cov}(X_t|\mathcal{I}_t)\hat{\text{R}}^{-1} \text{A}\mathbf{\Gamma},
\end{aligned} \right.
\end{equation}
где $\text{A}=\text{A}(\theta^{(\tau-1)})$, $\mathbf{\Gamma}=\mathbf{\Gamma}^{(\tau-1)}$, $\hat{\text{R}}=\hat{\text{R}}^{(\tau-1)}$. Для вывода этих формул используются свойство башни условного математического ожидания (tower property) и закон полной дисперсии (подробности приведены в приложении 2). \\
Оценим $\Expect_{(X_{t,m_t},S_t)|\mathcal{I}_t}[S_t S_t^*]$:
\begin{equation}
\Expect_{(X_{t,m_t},S_t)|\mathcal{I}_t}[S_t S_t^*] = \Expect[S_t|\mathcal{I}_t, X_{t,m_t}] \cdot \Expect[S_t^*|\mathcal{I}_t, X_{t,m_t}] + \text{Cov}(S_t|\mathcal{I}_t, X_{t, m_t}),
\end{equation}
Оценим $\Expect_{(X_{t,m_t},S_t)|\mathcal{I}_t}[X_t S_t^*]$:
\begin{equation}
\begin{gathered}
\Expect_{(X_{t,m_t},S_t)|\mathcal{I}_t}[X_t S_t^*] = \Expect[X_t X_t^*|\mathcal{I}_t]\hat{\text{R}}^{-1}\text{A}\mathbf{\Gamma},
\end{gathered}
\end{equation}
где $\text{A}=\text{A}(\theta^{(\tau-1)})$, $\hat{\text{R}}=\hat{\text{R}}^{(\tau-1)}$, $\mathbf{\Gamma}=\mathbf{\Gamma}^{(\tau-1)}$. \\
\begin{center}
\fontsize{12}{16}\selectfont \color{Purple}{\textbf{Апостериорное распределение латентных переменных для полных наблюдений}}
\end{center}
Если $m_t = \varnothing$, то апостериорное распределение упрощается следующим образом: $P(X_{t,m_t},S_t|\mathcal{I}_t)=P(S_t|\mathcal{I}_t)$.
Параметры апостериорного распределения $P(S_t|\mathcal{I}_t), t = 1,\ldots, \text{T}$:
\begin{equation}
\left\{ \begin{aligned} 
\Expect[S_t|\mathcal{I}_t] &= \mathbf{\Gamma}^*\text{A}^* \hat{\text{R}}^{-1}\Expect [X_t | \mathcal{I}_t], \\
\text{Cov}(S_t|\mathcal{I}_t) &= \mathbf{\Gamma} - \mathbf{\Gamma}^*\text{A}^*  \hat{\text{R}}^{-1}\text{A}\mathbf{\Gamma}.
\end{aligned} \right.
\end{equation}
При этом:
\begin{equation}
\Expect_{S_t|\mathcal{I}_t}[X_t X_t^*] = \Expect[X_t|\mathcal{I}_t] \cdot \Expect[X_t^*|\mathcal{I}_t] = x_t x_t^*,
\end{equation}
\begin{equation}
\Expect_{S_t|\mathcal{I}_t}[X_t S_t^*] = \Expect[X_t|\mathcal{I}_t] \cdot \Expect[S_t^*|\mathcal{I}_t] = x_t \cdot \Expect[S_t^*|\mathcal{I}_t] = x_t \cdot x_t^* \cdot \hat{\text{R}}^{-1} \text{A} \mathbf{\Gamma},
\end{equation}
\begin{equation}
\Expect_{S_t|\mathcal{I}_t}[S_t S_t^*] = \Expect[S_t|\mathcal{I}_t] \cdot \Expect[S_t^*|\mathcal{I}_t] + \text{Cov}(S_t|\mathcal{I}_t). 
\end{equation}
\clearpage
\begin{center}
\fontsize{12}{16}\selectfont \color{Purple}{\textbf{Агрегация}}
\end{center}
Теперь оценим вторые начальные моменты  $\Expect_{(X_m,S)|\mathcal{I}}[X X^*]$,  $\Expect_{(X_m,S)|\mathcal{I}}[S S^*]$, $\Expect_{(X_m,S)|\mathcal{I}}[X S^*]$:
\begin{equation}
\Expect_{(X_m,S)|\mathcal{I}}[XX^*] = \frac{1}{\text{T}}\sum_{t=1}^T \Expect[X_t X_t^*|\mathcal{I}_t],
\end{equation}
\begin{equation}
\Expect_{(X_m,S)|\mathcal{I}}[X S^*] =  \frac{1}{\text{T}} \Big[ \sum_{t \in T_1} \Expect_{(X_{t,m_t},S_t)|\mathcal{I}_t}[S_t S_t^*] + \sum_{t \in T_2} \Expect_{S_t|\mathcal{I}_t}[S_t S_t^*] \Big],
\end{equation}
\begin{equation}
\Expect_{(X_m,S)|\mathcal{I}}[X S^*] =  \frac{1}{\text{T}} \Big[ \sum_{t \in T_1} \Expect_{(X_{t,m_t},S_t)|\mathcal{I}_t}[X_t S_t^*] + \sum_{t \in T_2} \Expect_{S_t|\mathcal{I}_t}[X_t S_t^*] \Big],
\end{equation}
где $T_1 = \{y \in \Natural: m_y \ne \varnothing, 1 \le y \le T\}$, $T_2 = \{y \in \Natural: m_y = \varnothing, 1 \le y \le T\}$.
\begin{center}
\fontsize{14}{18}\selectfont \color{red}{\textbf{M-шаг}}
\end{center}
Требуется найти наилучшую оценку параметров, решив следующую задачу оптимизации:
\begin{equation*}
\begin{gathered}
\Upsilon^{(\tau)}=\argmax_{\Upsilon} \Expect_{(X_m,S)|\mathcal{I}}[\log P(X, S)] = \\
\argmin_{\Upsilon}  \text{T} \Big[\log|\mathbf{\Lambda}| +\Tr \big( \mathbf{\Lambda}^{-1} \Expect_{(X_m,S)|\mathcal{I}}[XX^*] \big) - 2 \Tr\big(\mathbf{\Lambda}^{-1} \text{A}(\theta) \Expect_{(X_m,S)|\mathcal{I}}[X S^*]\big) \\
+ \Tr(\mathbf{\Lambda}^{-1} \text{A}(\theta) \Expect_{(X_m,S)|\mathcal{I}}[SS^*] \text{A}^*(\theta)) + \log |\mathbf{\Gamma}| + \Tr(\mathbf{\Gamma}^{-1} \Expect_{(X_m,S)|\mathcal{I}}[SS^*]) \Big].
\end{gathered}
\end{equation*}
Оценим угловые координаты источников $\theta$:
\begin{equation*}
\begin{gathered}
\theta^{(\tau)}= \argmin_{\theta}\mathcal{Q}_1(\theta|\theta^{(\tau-1)})  = \argmin_{\theta} \Big[ - 2 \Tr\big(\mathbf{\Lambda}^{-1} \text{A}(\theta) \Expect_{(X_m,S)|\mathcal{I}}[X S^*]\big) \\
+ \Tr(\mathbf{\Lambda}^{-1} \text{A}(\theta) \Expect_{(X_m,S)|\mathcal{I}}[S S^*] \text{A}^*(\theta)) \Big] = \\
\argmin_{\theta} ||\mathbf{\Lambda}^{-1/2} (\Expect_{(X_m,S)|\mathcal{I}}[X S^*]-\text{A}(\theta)\Expect_{(X_m,S)|\mathcal{I}}[S S^*]) ||_F^2.
\end{gathered}
\end{equation*}
Эту задачу можно решить численно, подробности приведены в приложении 3. \\
Оценим ковариацию сигналов $\mathbf{\Gamma}$ с учетом предположения о некоррелированности сигналов:
\begin{equation*}
\begin{gathered}
\mathbf{\Gamma}^{(\tau)}= \argmin_{\mathbf{\Gamma}} \mathcal{Q}_2(\mathbf{\Gamma} | \mathbf{\Gamma}^{(\tau-1)})
 = \text{T}\bigg[ \log |\mathbf{\Gamma}| + \Tr(\mathbf{\Gamma}^{-1}\Expect_{(X_m,S)|\mathcal{I}}[S S^*])\bigg] = \mathcal{D} \Big[\Expect_{(X_m,S)|\mathcal{I}}[S S^*] \Big].
\end{gathered}
\end{equation*}
Обновляем оценку ковариации наблюдений с учетом полученных оценок параметров:
\begin{equation}
\hat{\text{R}}^{(\tau)} = \text{A}(\theta^{(\tau)})\mathbf{\Gamma}^{(\tau)}\text{A}^*(\theta^{(\tau)}) + \mathbf{\Lambda}.
\end{equation}
\clearpage
\begin{center}
\fontsize{16}{20}\selectfont \color{teal}{\textbf{Результаты численного эксперимента}}
\end{center}
\begin{center}
\fontsize{18}{23}\selectfont \color{black}{\textbf{Первые результаты, предварительные}}
\end{center}
\begin{center}
\fontsize{13}{16}\selectfont \color{purple}{\textbf{Первый набор начальных условий}}
\end{center}
\begin{equation*}
\begin{gathered}
L = 25, K = 1, T = 12, \text{5 сенсоров неисправны, 50\% пропусков для каждого из них}, \\ P = 0.5 \cdot E_{K}, Q = 8.1 \cdot E_{L}, \theta = [0.7] \approx [40.107^{\circ}].
\end{gathered}
\end{equation*}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{Screens/Angles1.png}
    \caption{График сходимости угловой координаты для первого набора начальных условий}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{Screens/likelihood1.png}
    \caption{График роста $\log(p(X_o|\Psi))$ для первого набора начальных условий}
\end{figure}
\clearpage
\begin{center}
\fontsize{13}{16}\selectfont \color{purple}{\textbf{Второй набор начальных условий}}
\end{center}
\begin{equation*}
\begin{gathered}
L = 25, K = 1, T = 11, \text{8 сенсоров неисправны, 50\% пропусков для каждого из них},\\  P = 0.5 \cdot E_{K}, Q = 6.1 \cdot E_{L}, \theta = [0.7] \approx [40.107^{\circ}].
\end{gathered}
\end{equation*}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{Screens/Angles2.png}
    \caption{График сходимости угловой координаты для второго набора начальных условий}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{Screens/likelihood2.png}
    \caption{График роста $\log(p(X_o|\Psi))$ для второго набора начальных условий}
\end{figure}
\clearpage
\begin{center}
\fontsize{16}{20}\selectfont \color{teal}{\textbf{Список источников}}
\end{center}
\begin{enumerate}
\item
Dempster A. P., Laird N. M., Rubin D. B. Maximum likelihood from incomplete data via the EM algorithm // Journal of the Royal Statistical Society: Series B (Methodological). – 1977. – Vol. 39. – No. 1.  – P. 1–38.
\item
Wu C. F. J. On the convergence properties of the EM algorithm // The Annals of Statistics. – 1983. – Vol. 11. –  No. 1. –  P. 95–103.
\item
Louis T. A. Finding the observed information matrix when using the EM algorithm // 
Journal of the Royal Statistical Society: Series B (Methodological). – 1982. – Vol. 44. – No. 2. – P. 226–233.
\item
Ross S. M. A first course in probability. 8th ed. – Upper Saddle River, N. J.: Prentice Hall, 2010. – 640p., p. 348.
\item
Little R. J. A., Rubin D. B. Statistical analysis with missing data 2nd ed. – Hoboken, N. J.: Wiley-Interscience, Jon Wiley \& Sons, Inc., 2002. – 381p.
\item
Schreier P. J., Scharf L. L. Statistical Signal Processing of Complex-Valued Data: The Theory of Improper and Noncircular Signals – Cambridge: Cambridge University Press, 2010. – 309p., pp. 41-42.
\item
Nocedal J., Wright S.J. Numerical Optimization. 2nd ed. – New York: Springer, 2006. –  664p., pp. 10-37.
\end{enumerate}
\clearpage
\begin{center}
\fontsize{16}{20}\selectfont \color{teal}{\textbf{Приложение №1. Инициализация}}
\end{center}
Оценим ковариационную матрицу $\hat{\text{R}}^{(0)}$ следующим образом: если $\sum_t \mathbf{I}_{x_t=x_{t,o_t}} \ge \text{L}$, оцениваем $\hat{\text{R}}^{(0)}$ по выборке, образованной $x_t: x_t=x_{t,o_t}$. В противном случае используется следующий подход: пусть $x^l$ -- строка матрицы $X$, соответствующая сенсору $l, l=1,\ldots,\text{L}$, $x_o^l$ -- выборка, образованная доступными наблюдениями в строке $l$, $x_m^l$ -- выборка, образованная недоступными наблюдениями в строке $l$, $x_t^l$ -- компонента наблюдения $x_t$, соответствующая сенсору $l$. Для всех недоступных значений построим следующую оценку: если $x_t^l \in x_m^l, l = 1,\ldots, \text{L},  t = 1,\ldots, \text{T}$, то $\hat{x}_t^l = \overline{x_o^l}$. Оцениваем ковариацию по такому оцененному набору $\hat{x}$:
\begin{equation}
\hat{\text{R}}^{(0)} = \text{Cov}(\hat{x}).
\end{equation}
\\
Оценим вектор угловых координат источников $\theta^{(0)}$ следующим образом:
\begin{enumerate}
\item
Выберем число $\nu$, которое будет соответствовать первому компоненту вектора $\theta^{(0)}$:
\begin{equation}
\nu \sim \mathcal{U}([-\pi;\pi]);
\end{equation}
\item
Оценим компоненты вектора $\theta^{(0)}$ так:  $\theta^{(0)}_i = (\nu + (i-1)\cdot \frac{2\pi}{\text{K}})\, \text{mod} \, 2\pi, i = 1,\ldots,\text{K}$. При этом,  $a \, \text{mod} \, b = a - b \cdot \lfloor \frac{a}{b} \rfloor$.
\end{enumerate}
Альтернативный подход: используем $\hat{\text{R}}^{(0)}$ для оценки  $\theta^{(0)}$ с помощью алгоритма MUSIC, используем эту оценку на первой итерации мультистарта ЕМ, на следующих используем оценки в окрестности указанной.\\
Начальную ковариацию сигналов $\mathbf{\Gamma}^{(0)}$ получаем на основе метода наименьших квадратов с помощью $\theta^{(0)}, \hat{\text{R}}^{(0)}$. Матрица $A(\theta^{(0)})$ -- матрица векторов направленности и представима в следующем виде:
\begin{equation}
A(\theta^{(0)})=[a(\theta^{(0)}_1), \ldots, a(\theta^{(0)}_K)],
\end{equation}
где $a(\theta^{(0)}_k)$ -- вектор направленности для источника $k$. Перед применением МНК каждый такой вектор отнормируем:
\begin{equation}
\check{a}(\theta^{(0)}_k) = \frac{a(\theta^{(0)}_k)}{||a(\theta^{(0)}_k)||}, k=1,\ldots,\text{K},
\end{equation}
сформируем из них нормированную матрицу векторов направленности:
\begin{equation}
\check{A}(\theta^{(0)})=[\check{a}(\theta^{(0)}_1), \ldots, \check{a}(\theta^{(0)}_\text{K})],
\end{equation}
применяем МНК для получения нормированной ковариации мощностей: 
\begin{equation}
\check{\Gamma}^{(0)}=\mathcal{D}\big[\check{A}(\theta^{(0)})^{+} (\hat{\text{R}}^{(0)}-\mathbf{\Lambda})(\check{A}(\theta^{(0)})^{+})^*\big],
\end{equation}
где $\mathcal{D}\big[ \cdot \big]$ -- диагональное приближение матрицы, $(\cdot)^{+}$ -- псевдообратная матрица. Такой подход не гарантирует, что все элементы матрицы на главной диагонали будут неотрицательными, поэтому используем следующую коректировку: $\check{\Gamma}_{ll}^{(0)} = \max(\check{\Gamma}_{ll}^{(0)}, \varepsilon)$, $l=1,\ldots,\text{L}$. Переходим к ненормированной мощности:
\begin{equation}
\mathbf{\Gamma}_{ll}^{(0)}=\frac{\check{\Gamma}_{ll}^{(0)}}{||a(\theta^{(0)}_k)||_l}, l=1,\ldots,\text{L}.
\end{equation}
Требуется гарантировать, что при выбранной инициализации на Е-шаге не будет получена условная ковариация сигналов, не являющаяся положительной определенной.
Вычисляем оценку условной ковариации сигналов, используя начальную оценку, и проверяем, выполняется ли свойство неотрицательной определенности:
\begin{equation}
\mathbf{\Gamma}^{(0)} - (\mathbf{\Gamma}^{(0)})^* \text{A}(\theta^{(0)})^*  (\hat{\text{R}}^{(0)})^{-1} \text{A}(\theta^{(0)})\mathbf{\Gamma}^{(0)} \succeq 0,
\end{equation} 
пока это свойство не выполняется, домножаем матрицу на 0.5: $\mathbf{\Gamma}^{(0)} = 0.5 \cdot \mathbf{\Gamma}^{(0)}$ и повторно проверяем выполненность вышеуказанного свойства.
\clearpage
\begin{center}
\fontsize{16}{20}\selectfont \color{teal}{\textbf{Приложение №2. Об апостериорном распределении сигналов, полученном на Е-шаге}}
\end{center}
В рамках Е-шага необходимо вычислить $\Expect[S_t|X_{t,o_t}=x_{t, o_t}]$,  $\text{Cov}(S_t|X_{t,o_t}=x_{t, o_t})$.
Вычисление $\Expect[S_t|X_{t,o_t}=x_{t, o_t}]$ реализуется так:
\begin{equation*}
\begin{gathered}
\Expect[S_t|X_{t,o_t}=x_{t, o_t}] =  \Expect[\Expect[S_t|X_t]|X_{t,o_t}=x_{t,o_t}]  \\ 
= \Expect[\mathbf{\Gamma}^* \text{A}^* \hat{\text{R}}^{-1} X_t|X_{t,o_t}=x_{t, o_t}] = \mathbf{\Gamma}^* \text{A}^* \hat{\text{R}}^{-1} \Expect[X_t|X_{t,o_t}=x_{t, o_t}].
\end{gathered}
\end{equation*}
Для нахождения $\text{Cov}(S_t|X_{t,o_t}=x_{t, o_t})$ может быть использовано равенство, называемое законом полной дисперсии (Law of total variance, может быть найдено в Sheldon Russ, A first course in probability, 8th ed., 2010):
\begin{equation}
\text{Cov}(W) = \Expect[\text{Cov}(W|Y)] + \text{Cov}(\Expect[W|Y]).
\end{equation}
Пусть $\mathcal{F}_Z=\sigma(Z)$: -- сигма-алгебра, порожденная $Z$. Тогда все числовые характеристики $W$, включая условные моменты и ковариации относительно $Y$, можно обусловить на $\mathcal{F}_Z$. И, соответственно, из закона полной ковариации следует:
\begin{equation}
\text{Cov}(W|\mathcal{F}_Z) = \Expect[\text{Cov}(W|Y,Z)|\mathcal{F}_Z] + \text{Cov}(\Expect[W|Y,Z]|\mathcal{F}_Z).
\end{equation}
Вектора $W$, $Y$, $Z$ предполагаются комплексными гауссовскими (случай круговой симметрии), зависимость между $Z$ и $Y$ линейная, $\sigma(Z) \subseteq \sigma(Y)$, предполагается, что $Z=CY$, где $C$ -- булев селектор.  Для линейной гауссовской модели знание $Z$ не уменьшает условную ковариацию $W|Y$ и не влияет на условное математическое ожидание $\Expect[W|Y,Z]$, поскольку $Z$ не содержит никакой новой информации, которой не было бы в $Y$:
\begin{equation*}
\begin{gathered}
\text{Cov}(W|Y,Z)=\text{Cov}(W|Y), \\
\Expect[W|Y,Z] = \Expect[W|Y].
\end{gathered}
\end{equation*}
Получаем:
\begin{equation}
\text{Cov}(W|\mathcal{F}_Z) = \Expect[\text{Cov}(W|Y)|\mathcal{F}_Z] + \text{Cov}(\Expect[W|Y]|\mathcal{F}_Z).
\end{equation}
Если $Z=z$, имеем:
\begin{equation}
\text{Cov}(W|Z=z) = \Expect[\text{Cov}(W|Y)|Z=z] + \text{Cov}(\Expect[W|Y]|Z=z).
\end{equation}
В рамках исходной задачи по оцениванию DoA и ковариации сигналов, $W$ соответствует величине $S_t$,  $Y$ соответствует величине $X_t$, $Z$ соответствует величине $X_{t, o_t}$,  $z$ соответствует величине $x_{t, o_t}, t=1,\ldots,\text{T}$. \\
Получаем:
\begin{equation}
\text{Cov}(S_t|X_{t,o_t}=x_{t, o_t}) = \Expect[\text{Cov}(S_t|X_t)|X_{t,o_t}=x_{t, o_t}] + \text{Cov}(\Expect[S_t|X_t]|X_{t,o_t}=x_{t, o_t}).
\end{equation}
Преобразуем первое слагаемое, учитывая тот факт, что ковариация не зависит от реализации $X_{t, o_t}$.
\begin{equation}
\Expect[\text{Cov}(S_t|X_t)|X_{t, o_t}=x_{t, o_t}] = \text{Cov}(S_t|X_t) = \mathbf{\Gamma} - \mathbf{\Gamma}^* \text{A}^* \hat{\text{R}}^{-1} \text{A} \mathbf{\Gamma}.
\end{equation}
Теперь преобразуем второе слагаемое:
\begin{equation*}
\begin{gathered}
\text{Cov}(\Expect[S_t|X_t]|X_{t, o_t}=x_{t, o_t}) = \text{Cov}(\mathbf{\Gamma}^* \text{A}^* \hat{\text{R}}^{-1} X_t |X_{t, o_t}=x_{t, o_t})  \\ 
= \mathbf{\Gamma}^* \text{A}^* \hat{\text{R}}^{-1} \text{Cov}(X_t | X_{t, o_t}=x_{t, o_t}) \hat{\text{R}}^{-1} \text{A} \mathbf{\Gamma}.
\end{gathered}
\end{equation*}
Таким образом:
\begin{equation}
\text{Cov}(S_t|X_{t,o_t}=x_{t, o_t}) = \mathbf{\Gamma} - \mathbf{\Gamma}^* \text{A}^* \hat{\text{R}}^{-1} \text{A} \mathbf{\Gamma} + \mathbf{\Gamma}^* \text{A}^* \hat{\text{R}}^{-1} \text{Cov}(X_t | X_{t, o_t}=x_{t, o_t}) \hat{\text{R}}^{-1} \text{A} \mathbf{\Gamma}.
\end{equation}
Для полных наблюдений $X_{t, o_t} = X_t$, и, соответственно:
\begin{equation}
\text{Cov}(\Expect[S_t|X_t]|X_{t, o_t}=x_{t, o_t})  = \text{Cov}(\Expect[S_t|X_t]|X_t=x_t) = 0. 
\end{equation}
О выводе смешанных вторых моментов $\Expect[X_t S_t^*|X_t=x_t]$ и $\Expect[X_t S_t^*|X_{t, o_t}=x_{t, o_t}]$:
\begin{equation}
\Expect[X_t S_t^*|X_t] = X_t \Expect[S_t^*|X_t] = X_t (\mathbf{\Gamma}^* \text{A}^* \hat{\text{R}}^{-1} X_t)^* = X_t X_t^* \hat{\text{R}}^{-1} \text{A} \mathbf{\Gamma},
\end{equation}
\begin{equation*}
\begin{gathered}
\Expect[X_t S_t^*|X_{t, o_t}=x_{t, o_t}] = \Expect[X_t \Expect[S_t^*|X_t]|X_{t, o_t}=x_{t, o_t}] = \Expect[X_t  (\mathbf{\Gamma}^* \text{A}^* \hat{\text{R}}^{-1} X_t)^*  | X_{t, o_t}=x_{t, o_t}] \\ 
= \Expect[X_t X_t^* \hat{\text{R}}^{-1} \text{A} \mathbf{\Gamma} |X_{t, o_t}=x_{t, o_t}] = \Expect[X_t X_t^*|X_{t, o_t}=x_{t, o_t}] \hat{\text{R}}^{-1} \text{A} \mathbf{\Gamma}
\end{gathered}
\end{equation*}
В приведенных выше переходах используется закон полного условного математического ожидания и уже полученное выражение для $\Expect[S_t^*|X_t]$.
\clearpage
\begin{center}
\fontsize{16}{20}\selectfont \color{teal}{\textbf{Приложение №3. О реализации М-шага}}
\end{center}
Если антенная решетка является равномерной и линейной, удобно искать оптимальный $u=\sin(\theta)$, а затем находить $\theta$ как $\arcsin(u)$. Для удобства введем новые обозначения: $C=\Expect_{(X_m,S)|\mathcal{I}}[X S^*], D=\Expect_{(X_m,S)|\mathcal{I}}[S S^*]$. Соответственно минимизации подлежит функция
\begin{equation}
\mathcal{Q}_1(u|u^{\tau-1)}) = ||\mathbf{\Lambda}^{-1/2} (C-\widetilde{A}(u)D) ||_F^2,
\end{equation}
где
\begin{equation}
{\small
\widetilde{A}(u) = \begin{bmatrix}
1&1&\dots&1\\
e^{-2j\pi \frac{\text{d}}{\lambda}u_1}& e^{-2j\pi \frac{\text{d}}{\lambda}u_2}&\dots&e^{-2j\pi \frac{\text{d}}{\lambda}u_\text{K}}\\
\dots&\dots&\ddots&\vdots\\
e^{-2j\pi (\text{L}-1) \frac{\text{d}}{\lambda}u_1}& e^{-2j\pi (\text{L}-1) \frac{\text{d}}{\lambda}u_2}&\dots&e^{-2j\pi (\text{L}-1) \frac{\text{d}}{\lambda}u_\text{K}}\\
\end{bmatrix}.
}
\end{equation}
Для ускорения оптимизации можно оптимизировать не $\mathcal{Q}_1(u|u^{(\tau-1)})$, а суррогатную функцию $\mathcal{G}(u|u^{(\tau-1)})$, построенную так:
\begin{equation}
\mathcal{G}(u|u^{(\tau-1)}) = \mathcal{Q}_1(u^{(\tau-1)}|u^{(\tau-1)}) + \grad \mathcal{Q}_1(u^{(\tau-1)}|u^{(\tau-1)})^T (u-u^{(\tau-1)}) + \frac{1}{2} (u-u^{(\tau-1)})^T \mathbf{H} (u-u^{(\tau-1)}),
\end{equation}
где
\begin{equation}
{\small
\mathbf{H} = \begin{bmatrix}
\max(|\grad_1 \mathcal{Q}_1(u^{(\tau-1)}|u^{(\tau-1)})|,\varepsilon)&0&\dots&0\\
0& \max(|\grad_2 \mathcal{Q}_1(u^{(\tau-1)}|u^{(\tau-1)})|,\varepsilon)&\dots&0\\
\dots&\dots&\ddots&\vdots\\
0& 0&\dots& \max(|\grad_\text{K} \mathcal{Q}_1(u^{(\tau-1)}|u^{(\tau-1)})|,\varepsilon)\\
\end{bmatrix},
}
\end{equation}
причем $\grad_1 \mathcal{Q}_1(u^{(\tau-1)}|u^{(\tau-1)})$ -- $i$-я компонента градиента $\mathcal{Q}_1(u^{(\tau-1)}|u^{(\tau-1)})$. Диагональная аппроксимация $\mathbf{H}$ предотвращает слишком маленькие или слишком большие шаги, сохраняя численную стабильность.
Теперь эту суррогатную функцию надо минимизировать (или, эквивалентно, найти направление шага $\rho^{(\tau)}=u-u^{(\tau-1)}$):
\begin{equation}
\min_u \mathcal{G}(u|u^{(\tau-1)}).
\end{equation}
Подставим $u = u^{(\tau-1)}+\rho$:
\begin{equation}
\mathcal{G}(u^{(\tau-1)} + \rho| u^{(\tau-1)}) = \mathcal{Q}_1(u^{(\tau-1)}| u^{(\tau-1)}) + \grad \mathcal{Q}_1 (u^{(\tau-1)}| u^{(\tau-1)})^T\rho + \frac{1}{2}\rho^T\mathbf{H}\rho.
\end{equation}
Для нахождения минимума берем градиент по $\rho$:
\begin{equation}
\grad_\rho \mathcal{G}(u^{(\tau-1)} + \rho| u^{(\tau-1)}) =  \grad \mathcal{Q}_1(u^{(\tau-1)}| u^{(\tau-1)})   + \mathbf{H}\rho,
\end{equation}
при $\grad_\rho \mathcal{G}=0$ будет выполняться:
\begin{equation}
0 =  \grad \mathcal{Q}_1(u^{(\tau-1)}| u^{(\tau-1)})   + \mathbf{H}\rho,
\end{equation}
решаем относительно $\rho^{(\tau)}$:
\begin{equation}
\rho^{(\tau)} = -\mathbf{H}^{-1}  \grad \mathcal{Q}_1(u^{(\tau-1)}| u^{(\tau-1)}).
\end{equation}
Важно заметить, что производная по направлению $\grad \mathcal{Q}_1(u^{(\tau-1)}| u^{(\tau-1)})^T \rho^{(\tau)}$ принимает вид:
\begin{equation}
\grad \mathcal{Q}_1(u^{(\tau-1)}| u^{(\tau-1)})^T \rho^{(\tau)} = - \grad \mathcal{Q}_1(u^{(\tau-1)}| u^{(\tau-1)})^T \mathbf{H}^{-1}  \grad \mathcal{Q}_1(u^{(\tau-1)}| u^{(\tau-1)}),
\end{equation} 
и поскольку $\mathbf{H}$ -- положительно определенная матрица, производная по направлению может принимать лишь неположительные значения, а значит направление $\rho^{(\tau)}$ соответствует невозрастанию функции.
Далее используется backtracking line search для гарантии невозрастания $\mathcal{Q}_1$:
\begin{equation}
u^{(\tau)}=u^{(\tau-1)} + \alpha_m \rho^{(\tau)},
\end{equation}
где $\alpha_m$ -- первое $\alpha=\alpha_0 \beta^m, m \in \Natural$, такое что $\mathcal{Q}_1(u^{(\tau)}| u^{(\tau-1)}) \le \mathcal{Q}_1(u^{(\tau-1)}| u^{(\tau-1)})$ для фиксированных $\alpha_0 > 0, \beta \in (0;1)$.\\
В рамках программного комплекса реализуется мультистарт для поиска оптимального $u$ на М-шаге. Указанный выше оптимизационный алгоритм запускается не только из точки $u^{(\tau-1)}$, но и из случайно выбранных допустимых точек, в том числе находящихся в окрестности $u^{(\tau-1)}$. Для выбранной начальной точки с индексом $j$, обновление оценки синусов угловых координат $\widetilde{u}^{(j)}$  может быть принято, только если:
\begin{enumerate}
\item
$\mathcal{Q}_1(\widetilde{u}^{(j)}| u^{(\tau-1)}) > \mathcal{Q}_1(u^{(\tau-1)}| u^{(\tau-1)})$,
\item
 $\mathcal{Q}_1(\widetilde{u}^{(j)}| u^{(\tau-1)}) > \mathcal{Q}_1(\min(\widetilde{u}^{(i)}: 1 \le i \le j-1)| u^{(\tau-1)})$, \text{при $j>1$}.
\end{enumerate}  
Оценим ковариацию сигналов $\mathbf{\Gamma}$:
\begin{equation}
\mathbf{\Gamma}^{(\tau)}= \argmin_{\mathbf{\Gamma}} \mathcal{Q}_2(\mathbf{\Gamma} | \mathbf{\Gamma}^{(\tau-1)}) = \text{T}\bigg[ \log |\mathbf{\Gamma}| + \Tr(\mathbf{\Gamma}^{-1}\Expect_{(X_m,S)|\mathcal{I}}[S S^*])\bigg].
\end{equation}
Определим точку, где производная данной функции принимает значение 0, и, таким образом, находим минимум функции:
\begin{equation*}
\begin{gathered}
\frac{\partial}{\partial \mathbf{\Gamma}}\log (\Det (\mathbf{\Gamma})) = \mathbf{\Gamma}^{-1}, \\
\frac{\partial}{\partial \mathbf{\Gamma}}\Tr(\mathbf{\Gamma}^{-1}\Expect_{(X_m,S)|\mathcal{I}}[S S^*])= -\mathbf{\Gamma}^{-1}\Expect_{(X_m,S)|\mathcal{I}}[S S^*]\mathbf{\Gamma}^{-1}, \\
\frac{\partial \mathcal{Q}_2(\mathbf{\Gamma})}{\partial \mathbf{\Gamma}} = \mathbf{\Gamma}^{-1}-\mathbf{\Gamma}^{-1}\Expect_{(X_m,S)|\mathcal{I}}[S S^*]\mathbf{\Gamma}^{-1}.
\end{gathered}
\end{equation*}
Приравняем производную к нулю (функция по $\mathbf{\Gamma}$ выпукла):
\begin{equation*}
\mathbf{O} = \mathbf{\Gamma}^{-1}-\mathbf{\Gamma}^{-1}\Expect_{(X_m,S)|\mathcal{I}}[S S^*]\mathbf{\Gamma}^{-1} \Rightarrow \mathbf{\Gamma}^{(\tau)} = \Expect_{(X_m,S)|\mathcal{I}}[S S^*].
\end{equation*}
Предполагая, что сигналы некоррелированны, будем использовать лишь диагональное приближение матрицы, приравняв элементы вне главной диагонали к нулю:
\begin{equation}
\mathbf{\Gamma}^{(\tau)} =  \mathcal{D} \Big[\Expect_{(X_m,S)|\mathcal{I}}[S S^*] \Big].
\end{equation}
Ввиду того, что М-шаг не максимизирует, а лишь улучшает УМО полного правдоподобия, представленный алгоритм является не точным ЕМ, а обобщенным (Generalized) EM.
\end{document}

