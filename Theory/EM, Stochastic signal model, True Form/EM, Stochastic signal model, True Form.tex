\documentclass[11pt]{article}
\usepackage[english,russian]{babel}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, left=2.5cm, right=1.5cm, top=2.5cm, bottom=2.5cm]{geometry}
\usepackage{animate} 
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{longtable}
\usepackage{amssymb}
\usepackage{physics}
\usepackage{tikz}
\usepackage{comment}
\usepackage{animate} 
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{longtable}
\usepackage{amssymb}
\usepackage{physics}
\usepackage{tikz}
\usepackage{comment}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage[normalem]{ulem}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{cancel}
\usepackage{mathtools}
\usepackage[most]{tcolorbox}
\usepackage[mathscr]{euscript}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
 
\newcommand{\Expect}{\mathbb{E}}
\newcommand{\Var}{\mathcal{D}}
\newcommand{\Cov}{\mathsf{cov}}
\newcommand{\Norm}{\mathcal{N}}
\newcommand{\NormComplex}{\mathcal{CN}}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\Complex}{\mathbb{C}}
\newcommand{\Int}{\mathbb{Z}}
\newcommand{\DK}{\mathbf{D}_{KL}}
\DeclarePairedDelimiterX{\infdivx}[2]{(}{)}{%
  #1\;\delimsize\|\;#2%
}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\Det}{Det}
\newcommand{\infdiv}{D_{KL}\infdivx}
\newcommand\Fontvi{\fontsize{8.2}{7.2}\selectfont}
\newcommand\Fontvia{\fontsize{9}{8}\selectfont}
\newcommand\Fontvib{\fontsize{10.8}{9.6}\selectfont}
\newcommand\Fontvic{\fontsize{8.0}{7.0}\selectfont}
\newcommand{\myitem}{\item[\checkmark]}
%\newcommand{\myitem}{\item[\squares]}

\begin{document}
\begin{center}
\fontsize{20}{23}\selectfont \color{red}{\textbf{ЕМ-алгоритм, стохастическая модель сигнала}}
\end{center}
\begin{center}
\fontsize{16}{20}\selectfont \color{teal}{\textbf{ \S1 Постановка проблемы}}
\end{center}
Предположим, имеется линейная антенная решетка, состоящая из $\text{L}$ сенсоров, которая принимает сигналы, направленные из $\text{K}$  источников, причем $\text{K} < \text{L}$. Этим источникам соответствуют угловые координаты (DoA) $\theta$, практически не изменяющиеся во времени. По итогам измерений было получено $\text{T}$ снимков полученного сигнала, причем ввиду стохастических технических сбоев, связанных с сенсорами, большая часть таких снимков содержит помимо надежных данных ненадежные, которые в рамках данной задачи рассматриваются как пропуски. Пусть $X$ --- набор наблюдений, полученных сенсорами в моменты времени $t=1,\ldots,\text{T}$, $X_t$ соответствует наблюдению в момент времени $t$, через $x$ и $x_t$ будем обозначать реализации полного набора наблюдений и наблюдения в отдельный момент времени $t$  соответственно. Ввиду наличия пропусков в данных, будем считать, что $X$ состоит из наблюдаемой части $X_o = \{X_{t, o_t}\}_{t=1}^\text{T}$ и ненаблюдаемой: $X_m = \{X_{t, m_t}\}_{t=1}^\text{T}$, причем $o_t \cup m_t = \{1,\ldots,\text{T}\}, o_t \cap m_t = \varnothing,  \forall t \in \{1,\ldots,\text{T}\}$. Предполагается, что $\nexists o_t: |o_t|=0$, т.е. нет таких наблюдений, которые состоят лишь из ненаблюдаемой части. \\ Набор наблюдений $X$ является результатом следующей модели наблюдений:
\begin{equation}
X = \text{A} S + N,
\end{equation}
где
\begin{equation}
X=[X_1,\ldots, X_\text{T}], S=[S_1,\ldots, S_\text{T}], N=[N_1,\ldots, N_\text{T}],
\end{equation}
и $X_t \in \Complex^{\text{L} \times \text{T}}, S_t \in \Complex^{\text{K} \times \text{T}}, N_t \in \Complex^{\text{L} \times \text{T}}$ -- векторы-столбцы, соответствующие наблюдениям, источникам и шумам в момент времени  $t = 1, \ldots, \text{T}$, $\text{A}$ -- матрица векторов направленности для равномерного линейного массива:
\begin{equation}
\text{A}(\theta) = \begin{bmatrix}
1&1&\dots&1\\
e^{-2j\pi \frac{\text{d}}{\lambda}\sin(\theta_1)}& e^{-2j\pi \frac{\text{d}}{\lambda}\sin(\theta_2)}&\dots&e^{-2j\pi \frac{\text{d}}{\lambda}\sin(\theta_\text{K})}\\
\dots&\dots&\ddots&\vdots\\
e^{-2j\pi (\text{L}-1) \frac{\text{d}}{\lambda}\sin(\theta_1)}& e^{-2j\pi (\text{L}-1) \frac{\text{d}}{\lambda}\sin(\theta_2)}&\dots&e^{-2j\pi (\text{L}-1) \frac{\text{d}}{\lambda}\sin(\theta_\text{K})}\\
\end{bmatrix}.
\nonumber
\end{equation}
Сигналы, испускаемые источниками, шумы на сенсорах и наблюдения предполагаются стохастическими: $S_t \sim \NormComplex(\mathbf{O}_{\text{K} \times 1},\mathbf{\Gamma})$, $N_t \sim \NormComplex(\mathbf{O}_{\text{L} \times 1}, \mathbf{\Lambda})$, $ X_t \sim \NormComplex(\mathbf{O}_{\text{L} \times 1}, \text{A}\mathbf{\Gamma}\text{A}^* + \mathbf{\Lambda})$, $t=1,\ldots,\text{T}$. Матрицы $\mathbf{\Gamma}$ и $\mathbf{\Lambda}$ предполагаются диагональными, т.е. сигналы не коррелированы между собой, шумы также не коррелированы между собой, корреляция между сигналами и шумами также отсутствует.
Составим EM-алгоритм для двух случаев:
\begin{itemize}
\item
Известный шум;
\item
Неизвестный шум.
\end{itemize}
\clearpage
\begin{center}
\fontsize{16}{20}\selectfont \color{teal}{\textbf{\S2 Известный шум}}
\end{center}
Воспользуемся EM-алгоритмом для того, чтобы определить значения параметров $\Psi = (\theta, \mathbf{\Gamma})$, пропущенные значения $X_m=\{X_{t, m_t}\}_{t=1}^\text{T}$ и сигналы $S$ рассматриваются как латентные переменные. Наблюдения $X_t$, $t=1,\ldots,\text{T}$ предполагаются независимыми и одинаково распределенными.
\begin{center}
\fontsize{14}{18}\selectfont \color{red}{\textbf{Инициализация параметров}}
\end{center}
Оценим вектор угловых координат источников $\theta^{(0)}$ следующим образом:
\begin{enumerate}
\item
Выберем число $\nu$, которое будет соответствовать первому компоненту вектора $\theta^{(0)}$:
\begin{equation}
\nu \sim \mathcal{U}([-\pi;\pi]);
\end{equation}
\item
Оценим компоненты вектора $\theta^{(0)}$ так:  $\theta^{(0)}_i = (\nu + (i-1)\cdot \frac{2\pi}{\text{K}})\, \text{mod} \, 2\pi, i = 1,\ldots,\text{K}$. При этом,  $a \, \text{mod} \, b = a - b \cdot \lfloor \frac{a}{b} \rfloor$.
\end{enumerate}
Начальную оценку ковариационной матрицы наблюдений $\hat{\text{R}}^{(0)}$ получаем на основе доступных данных. Начальную ковариацию сигналов $\Gamma^{(0)}$ получаем на основе метода наименьших квадратов с помощью $\theta^{(0)}, \hat{\text{R}}^{(0)}$.
\begin{center}
\fontsize{14}{18}\selectfont \color{red}{\textbf{Е-шаг}}
\end{center}
Требуется найти математическое ожидание полного правдоподобия с учетом  текущей оценки параметров и апостериорного совместного распределения пропущенных значений в наблюдениях $X_m$ и сигналов $S$ 
\begin{equation}
 \Expect_{(X_m,S)|X_o=x_o, \Psi^{(\tau-1)}}[\log P(X, S)].
\end{equation}
Преобразуем выражение, обозначив через $\mathcal{I}$ условную информацию $X_o=x_o, \Psi^{(\tau-1)}$, учитывая тот факт, что наблюдения являются независимыми:
\begin{equation*}
\begin{gathered}
 \Expect_{(X_m,S)|\mathcal{I}}[\log P(X, S)] = \\
 \Expect_{(X_m,S)|\mathcal{I}}[\log( P(X|S) P(S))] = \\
 \Expect_{(X_m,S)|\mathcal{I}}[\log P(X|S) + \log P(S)] = \\
\Expect_{(X_m,S)|\mathcal{I}}\Big[\sum_{t=1}^T \log   P(X_t|S_t) + \sum_{t=1}^T \log P(S_t)\Big] = \\
\Big[\sum_{t=1}^T  \Expect_{(X_{t,m_t},S_t)|\mathcal{I}_t} \log [P(X_t|S_t)] + \sum_{t=1}^T \Expect_{(X_{t,m_t},S_t)|\mathcal{I}_t}[\log P(S_t)]\Big] = \\
-\Big[ \text{T} \log|\mathbf{\Lambda}| +\Tr \big( \mathbf{\Lambda}^{-1} \Expect_{(X_m,S)|\mathcal{I}}[XX^*] \big) - 2 \Tr\big(\mathbf{\Lambda}^{-1} \text{A}(\theta) \Expect_{(X_m,S)|\mathcal{I}}[X S^*]\big) \\
+ \Tr(\mathbf{\Lambda}^{-1} \text{A}(\theta) \Expect_{(X_m,S)|\mathcal{I}}[SS^*] \text{A}^*(\theta)) + \text{T}  \log |\mathbf{\Gamma}| + \Tr(\mathbf{\Gamma}^{-1} \Expect_{(X_m,S)|\mathcal{I}}[SS^*]) \Big].
\end{gathered}
\end{equation*}
Для нахождения условных моментов, указанных в формуле выше, требуется найти апостериорное распределение скрытых переменных. Воспользуемся формулой произведения плотностей:
\begin{gather}
P(X_m,S|\mathcal{I}) = P(X_m|\mathcal{I}) \cdot P(S|X_m, \mathcal{I}).
\end{gather}
Сначала найдем апостериорное распределение $P(X_m|\mathcal{I})$, причем ввиду того, что индексы, соответствующие пропущенным значениям в наблюдениях, могут отличаться в зависимости от t, будем находить апостериорное распределение для каждого $X_{t, m_t}$. Для достижения этой цели, для каждой пары $ \{ (o_t, m_t): m_t \ne \varnothing \}$ создадим разбиение оценки ковариационной матрицы наблюдений $\hat{\text{R}}$ на блоки, индуцированное этим разбиением множества индексов, оно имеет следующий вид:
\begin{equation}
\hat{\text{R}} =
\begin{pmatrix}
\hat{\text{R}}_{o_t, o_t} & \hat{\text{R}}_{o_t, m_t}\\
\hat{\text{R}}_{m_t, o_t} & \hat{\text{R}}_{m_t, m_t}
\end{pmatrix},
\end{equation}
где каждый блок определяется как
\begin{equation*}
\hat{\text{R}}_{a,b} = (\hat{\text{R}}_{ij})_{i \in a, j \in b}.
\end{equation*}
Для каждого наблюдения, содержащего пропуски, требуется найти апостериорное распределение пропущенных значений, $P(X_{t, m_t}|X_{t, o_t}=x_{t, o_t},\Psi^{(\tau-1)}), t=1,\ldots,\text{T}, |m_t|>0$ . Обозначим через $\mathcal{I}_t$ условную информацию $X_{t, o_t}=x_{t, o_t},\Psi^{(\tau-1)}$.
Параметры апостериорного распределения $P(X_{t, m_t}|\mathcal{I}_t), t=1,\ldots,\text{T}$ на итерации $\tau$ можно найти следующим образом:
\begin{equation}
\left\{ \begin{aligned} 
\mu_{X_{t, m_t}|\mathcal{I}_t} &= \hat{\text{R}}_{m_t, o_t}\left(\hat{\text{R}}_{o_t, o_t}\right)^{-1}\cdot x_{t, o_t}, \\
\Sigma_{X_{t, m_t}|\mathcal{I}_t} &= \hat{\text{R}}_{m_t, m_t}-\hat{\text{R}}_{m_t, o_t}\left(\hat{\text{R}}_{o_t, o_t}\right)^{-1}\hat{\text{R}}_{o_t, m_t},
\end{aligned} \right.
\end{equation}
где $\hat{\text{R}}_{o_t, o_t}= \hat{\text{R}}_{o_t, o_t}^{(\tau-1)}$, 
$\hat{\text{R}}_{o_t, m_t}= \hat{\text{R}}_{o_t, m_t}^{(\tau-1)}$, 
$\hat{\text{R}}_{m_t, o_t}= \hat{\text{R}}_{m_t, o_t}^{(\tau-1)}$,
$\hat{\text{R}}_{m_t, m_t}= \hat{\text{R}}_{m_t, m_t}^{(\tau-1)}$.\\
Для каждого наблюдения $X_t$ оценим условную ковариационную матрицу $\widetilde{\Sigma}_{X_t} =\Expect_{X_{m_t}|\mathcal{I}_t}[X_t X_t^*]$:
\begin{equation*}
\begin{gathered}
\widetilde{\Sigma}_{X_t} = 
\begin{pmatrix}
\mu_{X_{t, o_t}|\mathcal{I}_t} \cdot \mu_{X_{t, o_t}|\mathcal{I}_t}^* & \mu_{X_{t, o_t}|\mathcal{I}_t} \cdot \mu^*_{X_{t, m_t}|\mathcal{I}_t}\\
\mu_{X_{t, m_t}|\mathcal{I}_t} \cdot \mu_{X_{t, o_t}|\mathcal{I}_t}^* & \mu_{X_{t, m_t}|\mathcal{I}_t} \cdot \mu^*_{X_{t, m_t}|\mathcal{I}_t}+ \Sigma_{X_{t, m_t}|\mathcal{I}_t} \\
\end{pmatrix} = \\
\Expect [X_t | \mathcal{I}_t] \cdot \Expect [X_t | \mathcal{I}_t]^* +
\begin{pmatrix}
\mathbf{O}_{o_t, o_t} & \mathbf{O}_{o_t, m_t} \\
\mathbf{O}_{m_t, o_t} & \Sigma_{X_{t, m_t}|\mathcal{I}_t} \\
\end{pmatrix},
\end{gathered}
\end{equation*}
где $\mathbf{O}_{o_t, o_t}$, $\mathbf{O}_{o_t, m_t}$, $\mathbf{O}_{m_t, o_t}$ -- нулевые блочные матрицы. Разбиение указанной матрицы на четыре блока, три из которых состоят из нулей, индукцировано разбиением множества индексов $\{1,\ldots,\text{L}\}$ на множества $o_t, m_t$. \\
Оценим условную ковариационную матрицу наблюдений $\widetilde{\Sigma}_X = \Expect_{(X_m,S)|\mathcal{I}}[XX^*]$:
\begin{equation}
\widetilde{\Sigma}_X = \frac{1}{\text{T}}\sum_{t=1}^T \widetilde{\Sigma}_{X_t}.
\end{equation}
Параметры апостериорного распределения $P(S_t|\mathcal{I}_t, X_{t, m_t}), t = 1,\ldots, \text{T}$ можно найти исходя из следующих формул, если $m_t \ne \varnothing$:
\begin{equation}
\left\{ \begin{aligned} 
\mu_{S_t|\mathcal{I}_t, X_{t, m_t}} &= \mathbf{\Gamma}\text{A}^* \hat{\text{R}}^{-1}\mu_{X_t|\mathcal{I}_t}, \\
\Sigma_{S_t|\mathcal{I}_t, X_{t, m_t}} &= \mathbf{\Gamma} - \mathbf{\Gamma}\text{A}^*  \hat{\text{R}}^{-1}\text{A}\mathbf{\Gamma} + \mathbf{\Gamma}\text{A}^* \hat{\text{R}}^{-1}\widetilde{\Sigma}_{X_t}\hat{\text{R}}^{-1} A\mathbf{\Gamma},
\end{aligned} \right.
\end{equation}
где $\text{A}=\text{A}(\theta^{(\tau-1)})$, $\mathbf{\Gamma}=\mathbf{\Gamma}^{(\tau-1)}$, $\hat{\text{R}}=\hat{\text{R}}^{(\tau-1)}$. \\
Если же $m_t = \varnothing$, то:
\begin{equation}
\left\{ \begin{aligned} 
\mu_{S_t|\mathcal{I}_t, X_{t, m_t}} &= \mathbf{\Gamma}\text{A}^* \hat{\text{R}}^{-1}\mu_{X_t|\mathcal{I}_t}, \\
\Sigma_{S_t|\mathcal{I}_t, X_{t, m_t}} &= \mathbf{\Gamma} - \mathbf{\Gamma}\text{A}^*  \hat{\text{R}}^{-1}\text{A}\mathbf{\Gamma}.
\end{aligned} \right.
\end{equation}
Оценим ковариационную матрицу сигналов с учетом текущей оценки параметров и доступных наблюдений $\widetilde{\Sigma}_S = \Expect_{(X_m,S)|\mathcal{I}}[SS^*]$. \\
\begin{equation}
\widetilde{\Sigma}_S =  \frac{1}{\text{T}} \Big[ \sum_{t=1}^T \big( \Sigma_{S_t|\mathcal{I}_t, X_{t, m_t}} + \mu_{S_t|\mathcal{I}_t, X_{t, m_t}} \cdot \mu_{S_t|\mathcal{I}_t, X_{t, m_t}}^H \big) \Big].
\end{equation}
Остается оценить кросс-ковариацию $\widetilde{\Sigma}_{XS}=\Expect_{(X_m,S)|\mathcal{I}}[X S^*]$:
\begin{equation}
\begin{gathered}
\Expect_{(X_{t,m_t},S_t)|\mathcal{I}_t}[X_t S_t^*] = \Expect_{(X_{t,m_t},S_t)|\mathcal{I}_t}[X_tX_t^*] (\hat{\text{R}}^{(\tau-1)})^{-1}\text{A}(\theta^{(\tau-1)})\mathbf{\Gamma}^{(\tau-1)} + \Expect[X_t|\mathcal{I}_t] \cdot \Expect[S_t|\mathcal{I}_t]^*\\
= \widetilde{\Sigma}_{X_t} (\hat{\text{R}}^{(\tau-1)})^{-1}\text{A}(\theta^{(\tau-1)})\mathbf{\Gamma}^{(\tau-1)} +  \mu_{X_t|\mathcal{I}_t}\cdot \mu_{S_t|\mathcal{I}_t, X_{t, m_t}}^*.
\end{gathered}
\end{equation}
\begin{equation}
\begin{gathered}
\widetilde{\Sigma}_{XS}= \Expect_{(X_m,S)|\mathcal{I}}[X S^*] = \frac{1}{\text{T}} \sum_{t=1}^T \Big( \widetilde{\Sigma}_{X_t} (\hat{\text{R}}^{(\tau-1)})^{-1}\text{A}(\theta^{(\tau-1)})\mathbf{\Gamma}^{(\tau-1)} +  \mu_{X_t|\mathcal{I}_t}\cdot \mu_{S_t|\mathcal{I}_t, X_{t, m_t}}^* \Big)
\end{gathered}
\end{equation}
\clearpage
\begin{center}
\fontsize{14}{18}\selectfont \color{red}{\textbf{M-шаг}}
\end{center}
Требуется найти наилучшую оценку параметров, решив следующую задачу оптимизации:
\begin{equation*}
\begin{gathered}
\Psi^{(\tau)}=\argmax_{\Psi} \Expect_{(X_m,S)|\mathcal{I}}[\log P(X, S)] = \\
\argmin_{\Psi}  \Big[\text{T} \log|\mathbf{\Lambda}| +\Tr \big( \mathbf{\Lambda}^{-1} \Expect_{(X_m,S)|\mathcal{I}}[XX^*] \big) - 2 \Tr\big(\mathbf{\Lambda}^{-1} \text{A}(\theta) \Expect_{(X_m,S)|\mathcal{I}}[X S^*]\big) \\
+ \Tr(\mathbf{\Lambda}^{-1} \text{A}(\theta) \Expect_{(X_m,S)|\mathcal{I}}[SS^*] \text{A}^*(\theta)) + \text{T} \log |\mathbf{\Gamma}| + \Tr(\mathbf{\Gamma}^{-1} \Expect_{(X_m,S)|\mathcal{I}}[SS^*]) \Big].
\end{gathered}
\end{equation*}
Оценим угловые координаты источников $\theta$:
\begin{equation*}
\begin{gathered}
\theta^{(\tau)}= \argmin_{\theta}\mathcal{J}(\theta)  = \argmin_{\theta} \Big[ - 2 \Tr\big(\mathbf{\Lambda}^{-1} \text{A}(\theta) \Sigma_{XS}\big) \\
+ \Tr(\mathbf{\Lambda}^{-1} \text{A}(\theta) \Sigma_S \text{A}^*(\theta)) \Big] = \\
\argmin_{\theta} ||\mathbf{\Lambda}^{-1/2} (\Sigma_{XS}-\text{A}(\theta)\Sigma_S) ||_F^2.
\end{gathered}
\end{equation*}
Для решения этой оптимизационной задачи можно использовать сочетание метода последовательного квадратичного программирования и линейного поиска. \\
Оценим ковариацию сигналов $\mathbf{\Gamma}$:
\begin{equation*}
\begin{gathered}
\mathbf{\Gamma}^{(\tau)}= \argmin_{\mathbf{\Gamma}} \mathcal{K}(\mathbf{\Gamma} | \mathbf{\Gamma}^{(\tau-1)})
 = \text{T}\bigg[ \log |\mathbf{\Gamma}| + \Tr(\mathbf{\Gamma}^{-1} \Expect_{(X_m,S)|\mathcal{I}}[SS^*])\bigg].
\end{gathered}
\end{equation*}
Определим точку, где производная данной функции принимает значение 0, и, таким образом, находим минимум функции,  при этом обозначим через $M$ величину $\Expect_{(X_m,S)|\mathcal{I}}[S_t S_t^* ]$:
\begin{equation*}
\begin{gathered}
\frac{\partial}{\partial \mathbf{\Gamma}}\log (\Det (\mathbf{\Gamma})) = \mathbf{\Gamma}^{-1}, \\
\frac{\partial}{\partial \mathbf{\Gamma}}\Tr(\mathbf{\Gamma}^{-1}M)= -\mathbf{\Gamma}^{-1}M\mathbf{\Gamma}^{-1}, \\
\frac{\partial \mathcal{K}(\mathbf{\Gamma})}{\partial \mathbf{\Gamma}} = \mathbf{\Gamma}^{-1}-\mathbf{\Gamma}^{-1}M\mathbf{\Gamma}^{-1}.
\end{gathered}
\end{equation*}
Приравняем производную к нулю (функция по $\mathbf{\Gamma}$ выпукла):
\begin{equation*}
O = \mathbf{\Gamma}^{-1}-\mathbf{\Gamma}^{-1}M\mathbf{\Gamma}^{-1} \Rightarrow M =\text{T}\mathbf{\Gamma} \Rightarrow \mathbf{\Gamma}^{(\tau)} = \widetilde{\Sigma}_S.
\end{equation*}
Предполагая, что сигналы некоррелированны, будем использовать лишь диагональное приближение матрицы, приравняв элементы вне главной диагонали к нулю:
\begin{equation}
\mathbf{\Gamma}^{(\tau)} =  \mathcal{D} \Big[\widetilde{\Sigma}_S \Big].
\end{equation}
Обновляем оценку ковариации наблюдений с учетом полученных оценок параметров:
\begin{equation}
\hat{\text{R}}^{(\tau)} = \text{A}(\theta^{(\tau)})\mathbf{\Gamma}\text{A}^*(\theta^{(\tau)}) + \mathbf{\Lambda}.
\end{equation}
\begin{center}
\fontsize{16}{20}\selectfont \color{teal}{\textbf{Список источников}}
\end{center}
\begin{enumerate}
\item
Dempster, A.P.; Laird, N.M.; Rubin, D.B. Maximum likelihood from incomplete data via the EM algorithm. J. R. Stat. Soc. Ser. B
(Methodol.) 1977, 39, 1–38.
\end{enumerate}
\end{document}

